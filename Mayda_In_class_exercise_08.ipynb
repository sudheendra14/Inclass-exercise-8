{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Mayda In_class_exercise_08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5f1pTgj7Ba2"
      },
      "source": [
        "# **The eighth in-class-exercise (20 points in total, 3/30/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOZbCpOa7BbF"
      },
      "source": [
        "The data for this exercise is from the dataset you created from assignment three. Please perform answer the following questions based on your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooDTv9T-7BbG"
      },
      "source": [
        "## (1) (10 points) Write a python program to extract the sentiment related terms from the corpus. You may use python package such as polyglot or external lexicon resources in the question. Rank the sentiment related terms by frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7sbHf1Q7BbH",
        "outputId": "9f54a9a4-c33e-4ab6-a55c-a9b8d7fe611e"
      },
      "source": [
        "# Write your code here\n",
        "!pip install TextBlob\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from TextBlob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->TextBlob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR6CPfAJU0c5"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/joker_data1.csv\")\n",
        "data.head(10)\n",
        "main_df=data[[\"review_content\"]]\n",
        "main_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y26oyqDFU4cx"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "#remove stop words \n",
        "from nltk.corpus import stopwords\n",
        "stopwords = list(stopwords.words('english'))\n",
        "\n",
        "#find the occurance of each word from the list\n",
        "main_df[\"Tokens\"] = main_df[\"review_content\"].apply(lambda x : nltk.word_tokenize(str(x)))\n",
        "main_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvLEr5TqU-Up"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "def compute_values(ip):\n",
        "  res1 =pd.value_counts(ip)\n",
        "  if(len(res1)==0 ):\n",
        "    res1 = [0]\n",
        "  return res1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVk4dV4wVDI4"
      },
      "source": [
        "df1 = df1.fillna(0)\n",
        "final_df = pd.DataFrame(df1.columns, columns=[\"Tokens\"])\n",
        "res = pd.DataFrame(df1.T.sum(axis=1),columns = [\"value\"]).values.tolist()\n",
        "final_df[\"value_of_tf\"] = pd.DataFrame(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmTb3p0kVLG8"
      },
      "source": [
        "final_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faD_PkkxVMmV"
      },
      "source": [
        "from textblob  import TextBlob\n",
        "#find the polarity \n",
        "final_df[\"Polarity\"] = final_df['Tokens'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "#filter the influencing variables \n",
        "iv = final_df[\"Polarity\"] != 0\n",
        "iv_filter = final_df[iv]\n",
        "iv_filter.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R8n5DF2VSV9"
      },
      "source": [
        "#ranks based on the occurance \n",
        "result = iv_filter.sort_values(by = [\"value_of_tf\"],ascending = False).reset_index()\n",
        "result[\"Rank\"] = pd.DataFrame(range(1,242))\n",
        "result.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EREmtgZ57BbJ"
      },
      "source": [
        "## (2) (10 points) Compare the performance of the following tools in sentiment identification: TextBlob (https://textblob.readthedocs.io/en/dev/), VADER (https://github.com/cjhutto/vaderSentiment), TFIDF-based Support Vector Machine (SVM) (Split your data into training and testing data). Take your own annotation as the standard answers. \n",
        "\n",
        "Reference code: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx2lRzmW7BbK"
      },
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/joker_data1.csv\",usecols=[\"review_content\",\"Sentiment\"])\n",
        "df=df.dropna().reset_index()\n",
        "df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozQU1RR3Vl-e"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "textblob_data = pd.read_csv(\"/content/joker_data1.csv\")\n",
        "textblob_data['polarity'] = textblob_data['review_content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "textblob_data['predicted sentiment'] = pd.cut(textblob_data['polarity'], bins=4, labels=[1, 2, 3, 4])\n",
        "#Defining sentiment\n",
        "def sentiment(x):\n",
        "    if x in [1, 2]:\n",
        "        return 'Negative'\n",
        "    if x == 3:\n",
        "        return 'Neutral'\n",
        "    if x == 4:\n",
        "        return 'Positive'\n",
        "textblob_data['predicted sentiment'] = textblob_data['predicted sentiment'].apply(lambda x: sentiment(x))\n",
        "print(textblob_data[['documentid', 'sentiment', 'predicted sentiment']].head(5))\n",
        "textblob_accuracy = accuracy_score(textblob_data['sentiment'], textblob_data['predicted sentiment'])*100\n",
        "textblob_f1 = f1_score(textblob_data['sentiment'], textblob_data['predicted sentiment'], average='macro')\n",
        "print(' f1-score of the TextBlob:', textblob_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZY4PEPIVlv5"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "myvader_data = pd.read_csv(\"/content/joker_data1.csv\")\n",
        "myvader_data['polarity'] = textblob_data['review_content'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "myvader_data['predicted sentiment'] = pd.cut(myvader_data['polarity'], bins=4, labels=[1, 2, 3, 4])\n",
        "myvader_data['predicted sentiment'] = myvader_data['predicted sentiment'].apply(lambda x: sentiment(x))\n",
        "print(myvader_data[['documentid', 'sentiment', 'predicted sentiment']].head(5))\n",
        "vader_accuracy = accuracy_score(myvader_data['sentiment'], myvader_data['predicted sentiment'])*100\n",
        "vader_f1 = f1_score(myvader_data['sentiment'], myvader_data['predicted sentiment'], average='macro')\n",
        "print(' f1-score of the VADER sentiment identification :', vader_f1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jtTDljsVuyj"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mysvm = pd.read_csv(\"/content/joker_data1.csv\")\n",
        "train, test = sklearn.model_selection.train_test_split(mysvm, train_size=0.6, test_size=0.1)\n",
        "mypip = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=100, \n",
        "                                           learning_rate='optimal', tol=None))])\n",
        "\n",
        "svm = mypip.fit(train['review_content'], train['sentiment'])\n",
        "test['predicted sentiment'] = svm.predict(test['review_content'])\n",
        "print(test[['documentid', 'sentiment', 'predicted sentiment']].head(5))\n",
        "\n",
        "mysvm_acc = accuracy_score(test['sentiment'], test['predicted sentiment'])*100\n",
        "mysvm_f1 = f1_score(test['sentiment'], test['predicted sentiment'], average='macro')\n",
        "\n",
        "print('Accuracy of the TFIDF-based SVM:', mysvm_acc)\n",
        "print('f1-score of the TFIDF-based SVM :', mysvm_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MReyziGLV1VI"
      },
      "source": [
        "Sentiment Analysis can be done by using \n",
        "1.TextBlob\n",
        "2.VADER \n",
        "3.TF-IDF based SVM. \n",
        "**TextBlob** is a well-known Python library for handling textual files. It's based on NLTK, another common Python Natural Language Processing toolbox.\n",
        "TextBlob awards scores for each word using a sentiment lexicon (a list of predefined words), which are then combined to create an overall sentence sentiment score.\n",
        "\n",
        "**VADER:**Another common rule-based library for sentiment analysis is VADER: Valence Aware Dictionary and sEntiment Reasoner.\n",
        "It, too, uses an emotion lexicon with severity measurements for each word dependent on human-annotated marks, similar to TextBlob. VADER, on the other hand, was created with an emphasis on social media.\n",
        "\n",
        "The **SVM:TF-IDF** is a numerical metric that calculates the value of a term in a text document.\n",
        "In text categorization, the Support Vector Machine (SVM) is one of the most widely used classification algorithms.\n",
        "With the aid of the kernel function, it can deal with both linear and non-linear data classification.\n",
        "\n",
        "The accuracy score and f1 score can be used to assess the models' results.\n",
        "\n",
        "**Analysis**: According to my findings, the TF-IDF Based SVM has the highest accuracy and f1 score, with 90 percent and 0.47, respectively, out of the three models I used for sentiment analysis. The simplest model for sentiment analysis is SVM.\n",
        "\n",
        "It was extremely difficult to distinguish between VADER and Textblob."
      ]
    }
  ]
}